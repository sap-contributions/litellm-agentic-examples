{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You need to run litellm proxy first.\n",
    "\n",
    "Install ag2 with openai"
   ],
   "id": "b63a1db1d250c535"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T10:21:25.870443Z",
     "start_time": "2025-11-04T10:21:25.481117Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install ag2[openai]",
   "id": "325dab9c1494fd65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ag2[openai]\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "or",
   "id": "b309325c9b96224e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T09:19:08.594245Z",
     "start_time": "2025-11-05T09:18:53.753691Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install 'ag2[openai]'",
   "id": "ca03c61aff5a08f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ag2[openai]\r\n",
      "  Using cached ag2-0.10.0-py3-none-any.whl.metadata (36 kB)\r\n",
      "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in /Users/vasilisa/PycharmProjects/litellm-agentic-examples/.venv/lib/python3.12/site-packages (from ag2[openai]) (4.11.0)\r\n",
      "Collecting diskcache (from ag2[openai])\r\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\r\n",
      "Collecting docker (from ag2[openai])\r\n",
      "  Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Requirement already satisfied: httpx<1,>=0.28.1 in /Users/vasilisa/PycharmProjects/litellm-agentic-examples/.venv/lib/python3.12/site-packages (from ag2[openai]) (0.28.1)\r\n",
      "Requirement already satisfied: packaging in /Users/vasilisa/PycharmProjects/litellm-agentic-examples/.venv/lib/python3.12/site-packages (from ag2[openai]) (25.0)\r\n",
      "Collecting pydantic<3,>=2.6.1 (from ag2[openai])\r\n",
      "  Using cached pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)\r\n",
      "Collecting python-dotenv (from ag2[openai])\r\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\r\n",
      "Collecting termcolor (from ag2[openai])\r\n",
      "  Using cached termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\r\n",
      "Collecting tiktoken (from ag2[openai])\r\n",
      "  Using cached tiktoken-0.12.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\r\n",
      "Collecting openai>=1.99.3 (from ag2[openai])\r\n",
      "  Downloading openai-2.7.1-py3-none-any.whl.metadata (29 kB)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/vasilisa/PycharmProjects/litellm-agentic-examples/.venv/lib/python3.12/site-packages (from anyio<5.0.0,>=3.0.0->ag2[openai]) (3.11)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/vasilisa/PycharmProjects/litellm-agentic-examples/.venv/lib/python3.12/site-packages (from anyio<5.0.0,>=3.0.0->ag2[openai]) (1.3.1)\r\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /Users/vasilisa/PycharmProjects/litellm-agentic-examples/.venv/lib/python3.12/site-packages (from anyio<5.0.0,>=3.0.0->ag2[openai]) (4.15.0)\r\n",
      "Requirement already satisfied: certifi in /Users/vasilisa/PycharmProjects/litellm-agentic-examples/.venv/lib/python3.12/site-packages (from httpx<1,>=0.28.1->ag2[openai]) (2025.10.5)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/vasilisa/PycharmProjects/litellm-agentic-examples/.venv/lib/python3.12/site-packages (from httpx<1,>=0.28.1->ag2[openai]) (1.0.9)\r\n",
      "Requirement already satisfied: h11>=0.16 in /Users/vasilisa/PycharmProjects/litellm-agentic-examples/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.28.1->ag2[openai]) (0.16.0)\r\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.99.3->ag2[openai])\r\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting jiter<1,>=0.10.0 (from openai>=1.99.3->ag2[openai])\r\n",
      "  Using cached jiter-0.11.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\r\n",
      "Collecting tqdm>4 (from openai>=1.99.3->ag2[openai])\r\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6.1->ag2[openai])\r\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\r\n",
      "Collecting pydantic-core==2.41.4 (from pydantic<3,>=2.6.1->ag2[openai])\r\n",
      "  Using cached pydantic_core-2.41.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.3 kB)\r\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=2.6.1->ag2[openai])\r\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/vasilisa/PycharmProjects/litellm-agentic-examples/.venv/lib/python3.12/site-packages (from docker->ag2[openai]) (2.32.5)\r\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/vasilisa/PycharmProjects/litellm-agentic-examples/.venv/lib/python3.12/site-packages (from docker->ag2[openai]) (2.5.0)\r\n",
      "Collecting regex>=2022.1.18 (from tiktoken->ag2[openai])\r\n",
      "  Downloading regex-2025.11.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/vasilisa/PycharmProjects/litellm-agentic-examples/.venv/lib/python3.12/site-packages (from requests>=2.26.0->docker->ag2[openai]) (3.4.4)\r\n",
      "Downloading openai-2.7.1-py3-none-any.whl (1.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m773.5 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached pydantic-2.12.3-py3-none-any.whl (462 kB)\r\n",
      "Using cached pydantic_core-2.41.4-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\r\n",
      "Using cached ag2-0.10.0-py3-none-any.whl (881 kB)\r\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n",
      "Using cached docker-7.1.0-py3-none-any.whl (147 kB)\r\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\r\n",
      "Using cached termcolor-3.2.0-py3-none-any.whl (7.7 kB)\r\n",
      "Using cached tiktoken-0.12.0-cp312-cp312-macosx_11_0_arm64.whl (994 kB)\r\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\r\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\r\n",
      "Using cached jiter-0.11.1-cp312-cp312-macosx_11_0_arm64.whl (315 kB)\r\n",
      "Downloading regex-2025.11.3-cp312-cp312-macosx_11_0_arm64.whl (288 kB)\r\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\r\n",
      "Installing collected packages: typing-inspection, tqdm, termcolor, regex, python-dotenv, pydantic-core, jiter, distro, diskcache, annotated-types, tiktoken, pydantic, docker, openai, ag2\r\n",
      "Successfully installed ag2-0.10.0 annotated-types-0.7.0 diskcache-5.6.3 distro-1.9.0 docker-7.1.0 jiter-0.11.1 openai-2.7.1 pydantic-2.12.3 pydantic-core-2.41.4 python-dotenv-1.2.1 regex-2025.11.3 termcolor-3.2.0 tiktoken-0.12.0 tqdm-4.67.1 typing-inspection-0.4.2\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For more information about ag2 framework do to [docs](https://docs.ag2.ai/latest/)",
   "id": "858f6b3463faf747"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import litellm\n",
    "from dotenv import load_dotenv\n",
    "from typing import Annotated, Any\n",
    "from autogen import ConversableAgent, LLMConfig\n",
    "from autogen.agentchat.group.patterns import AutoPattern\n",
    "from autogen.agentchat import initiate_group_chat"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Set env variables:\n",
    "- your litellm proxy master key\n",
    "- your litellm proxy url\n",
    "- set use_litellm_proxy True"
   ],
   "id": "73a3e1d40d4cd663"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca983253",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "litellm.use_litellm_proxy = True\n",
    "load_dotenv()\n",
    "api_base = os.getenv(\"LITELLM_ROXY_URL\")\n",
    "api_key = os.getenv(\"LITELLM_MASTER_KEY\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define the tool function",
   "id": "58d4e9f20bcf322b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5debdb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Moke function\"\"\"\n",
    "    city_normalized = city.lower().replace(\" \", \"\")\n",
    "\n",
    "    mock_weather_db = {\n",
    "        \"newyork\": \"The weather in New York is sunny with a temperature of 25°C.\",\n",
    "        \"london\": \"It's cloudy in London with a temperature of 15°C.\",\n",
    "        \"tokyo\": \"Tokyo is experiencing light rain and a temperature of 18°C.\",\n",
    "    }\n",
    "\n",
    "    if city_normalized in mock_weather_db:\n",
    "        return mock_weather_db[city_normalized]\n",
    "    else:\n",
    "        return f\"The weather in {city} is sunny with a temperature of 20°C.\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set up the model with your proxy params",
   "id": "ba1cb106a07d2c91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b2e2d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": "llm_config = LLMConfig(config_list={\"model\": \"sap/gpt-4o\", \"base_url\": api_base, \"api_key\": api_key})"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define the function for check if agent conversation is finished",
   "id": "f0a8291971a73884"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a11076",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def is_termination_msg(msg: dict[str, Any]) -> bool:\n",
    "    content = msg.get(\"content\", \"\")\n",
    "    return (content is not None) and \"==== REPORT GENERATED ====\" in content"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set up an agent with created model and tool, put the system message if needed",
   "id": "dcb445ad60e2b732"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc2754f",
   "metadata": {},
   "source": [
    "assistant = ConversableAgent(name=\"assistant\",\n",
    "                             llm_config=llm_config,\n",
    "                             system_message=\"You are a helpful weather assistant. \"\n",
    "                                            \"When the user asks for the weather in a specific city, \"\n",
    "                                            \"use the 'get_weather' tool to find the information. \"\n",
    "                                            \"If the tool returns an error, inform the user politely. \"\n",
    "                                            \"If the tool is successful, write a couple sentences for \"\n",
    "                                            \"TV weather report in the city, that will be include small jok.\"\n",
    "                                            \"Once you've generated the report append the below in the summary:\"\n",
    "                                            \"==== REPORT GENERATED ====\",\n",
    "                             functions=[get_weather])"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set up the conversation pattern with created agent, model and function for conversation termination",
   "id": "12d0d4b3b39c51b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pattern = AutoPattern(initial_agent=assistant,\n",
    "                      agents=[assistant],\n",
    "                      group_manager_args={\n",
    "                          \"llm_config\": llm_config,\n",
    "                          \"is_termination_msg\": is_termination_msg\n",
    "                      },\n",
    "                      )"
   ],
   "id": "1e0535bad5b9abde",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the conversation with message",
   "id": "c7f923590921a102"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "result, _, _ = initiate_group_chat(pattern=pattern,\n",
    "                                   messages=\"What is the weather like in Tbilisi?\",\n",
    "                                   )"
   ],
   "id": "1f981e754e2a2dbb"
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
